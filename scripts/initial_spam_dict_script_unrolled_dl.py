# -*- coding: utf-8 -*-
"""script_unrolled_DL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w1TfUcqDPpEbSdIB7EViSZpQDGwkmTfr
"""

import pandas as pd
import torch
import torch.nn.functional as F
import torchvision
#import spams
#import scanpy
import os
import json
import pickle
import numpy as np
from datetime import datetime
import argparse
from tqdm import tqdm, trange
import scipy 
from torch.utils.data import Dataset
from scipy.optimize import linear_sum_assignment as lsa
import matplotlib.pyplot as plt
import scipy.io
#from numba import jit, cuda

# !pip install tensorboardX
import tensorboardX

# import os
# from google.colab import drive

# # Mount your Google Drive
# drive.mount('/content/drive')

# # Set the current working directory
# os.chdir('/content/drive/MyDrive/AM231_project/data')


os.chdir('/content/drive/MyDrive/AM231_project/data')


# parameters related to the dataset
processing_params = {
    'overload_type':'guide', # guide by 'default' or 'droplet'
    'inputs_processed_X':['guide_pooled_processed_exp.npy','conventional_processed_exp.npy'],
    'input_perturbation_matrix': ['GSM6858450_KD_guide_pooled_perturbations.txt','GSM6858447_KO_conventional_perturbations.txt'],
    'logmeanexp': ['guide_pooled_processed_logmeanexp.npy','conventional_processed_logmeanexp.npy'],
    'genes': ['guide_pooled_processed_gene_names.txt','conventional_processed_gene_names.txt'],
    'cells': ['guide_pooled_processed_cell_names.txt','conventional_processed_cell_names.txt'],
    'timestamp':datetime.now().strftime("%Y_%m_%d_%H_%M_%S"),
    'number_top_effect_sizes': 1000,
    'method': ['KD_guide_pooled','KO_conventional'],
    'Laplacian_cells':['L_guide_pooled_cells_X.mat','L_conventional_cells_X.mat'],
    'Laplacian_genes':['L_guide_pooled_genes_X.mat','L_conventional_genes_X.mat'],
    'Laplacian_cells_conventional':'L_conventional_cells_X.mat',
    'Laplacian_genes_conventional':'L_conventional_genes_X.mat',
}

parser = argparse.ArgumentParser()
group = parser.add_mutually_exclusive_group()
parser.add_argument("--guide_pool", action='store_true', default=False)
parser.add_argument("--conventional", action='store_true', default=False)
parser.add_argument("--descr_file", type=str, default="")
parser.add_argument("--eps_Adam", type=float, default=1e-08) # eps adam to intially set
parser.add_argument("--delay_eps_Adam", type=float) # delayed eps adam to set if --delay_Adam_opt is True
parser.add_argument("--delay_Adam_opt", action='store_true', default=False)
parser.add_argument("--laplacian_genes", action='store_true', default=False)
parser.add_argument("--laplacian_cells", action='store_true', default=False)
parser.add_argument("--lambda_L_genes_reg", type=float, default=0) # 1000 3000 5000
parser.add_argument("--lambda_L_cells_reg", type=float, default=0)
parser.add_argument("--epochs", type=int, default=500)
parser.add_argument("--milestones", type=list, default=[400]) # --milestones 400 450 475 
parser.add_argument("--initial_spam_dictpath", type=str) # path to spams dictionary
args = parser.parse_args()

# parameters related to pytorch
params = {

    "device": "cuda:0" if torch.cuda.is_available() else "cpu",
    "random_date": datetime.now().strftime("%Y_%m_%d_%H_%M_%S"),
    "num_data": None, # number of cells
    "num_genes": None,  # number of genes
    "num_modules": 10,  # number of modules
    # training parameters
    "code_l1_reg_net": 0.10, # lambda1 in forward pass for lasso
    "code_l1_reg_loss": 0.10, # lambda1 in the loss for lasso
    "code_l2_reg": 0, 
    "dict_l1_reg": 0,
    "dict_l2_reg": 0,
    "lambda_L_cells_reg": args.lambda_L_cells_reg, # lambda on prior related to cells
    "lambda_L_genes_reg": args.lambda_L_genes_reg, # lambda on prior related to genes
    "alpha": 0.5, # more modules, larger dimensionality of code, smaller alpha = step size in forward mode 
    # alpha = 1/L where L should be greater than largest singular value of the dictionary
    "enc_layers": 200,
    "non_neg_code": False,
    "batch_size": 512,
    "normalize": True,
    # related to the optimizer
    "lr_lasso": 1e-3,
    "lr_laplacian": 1e-4,
    "lasso_update_period": 20,
    "laplacian_update_period": 1,
    "num_workers": 1,
    "num_epochs": args.epochs,
    "shuffle": True,
    "train_val_split": 1, # percent of samples to put in test set
    "random_split_manual_seed": 11,
    "percent_genes_train": 0.15, 
    "laplacian_flag": False,
    "descr": args.descr_file,
    "eps_Adam": args.eps_Adam,
    "delay_Adam_opt": args.delay_Adam_opt,
    "delay_eps_Adam": args.delay_eps_Adam,
    "milestones": args.milestones,
}

if args.guide_pool:
  params['name_input_data'] = 'guide_pooled'
  params['dataset_index'] = 0

  # read in Laplacians
  if args.laplacian_genes:
    L_genes = torch.tensor(scipy.io.loadmat(processing_params['Laplacian_genes'][0])['L_full_data'].todense(), dtype=torch.float32 )
    params['laplacian_flag'] = True

  if args.laplacian_cells:
    L_cells = scipy.io.loadmat(processing_params['Laplacian_cells'][0])['L_manifold'].todense()
    params['laplacian_flag'] = True


if args.conventional:
  params['name_input_data'] = 'conventional'
  params['dataset_index'] = 1

  # read in Laplacians
  if args.laplacian_genes:
    L_genes = torch.tensor(scipy.io.loadmat(processing_params['Laplacian_genes'][1])['L_full_data'].todense(), dtype=torch.float32 )
    params['laplacian_flag'] = True

  if args.laplacian_cells:
    L_cells = scipy.io.loadmat(processing_params['Laplacian_cells'][1])['L_manifold'].todense()
    params['laplacian_flag'] = True

path = '/content/drive/MyDrive/AM231_project/data/'
train_log_dir = 'logs/tensorboard/train/' + params['name_input_data'] + '_' + params['descr'] + '_lambda_'+ str(params['code_l1_reg_net']) + '_laplacian_' + str(params['laplacian_flag']) + '_epochs_' + str(params['num_epochs']) + '_' + params['random_date']

tensorboardx_path = os.path.join(path, train_log_dir)
os.mkdir(tensorboardx_path)

writer = tensorboardX.SummaryWriter(tensorboardx_path)
writer.add_text("params", str(params))
writer.flush()

print("Constructing pytorch model...")

class LassoLoss(torch.nn.Module):
    def __init__(self, lam):
        super(LassoLoss, self).__init__()
        self.lam = lam

    def forward(self, x, xhat, zhat):

        rec = 0.5 * (x - xhat).pow(2).sum(dim=1).mean()

        l1z = zhat.abs().sum(dim=1).mean()

        return rec + self.lam * l1z 
        
class LaplacianLoss_Genes(torch.nn.Module):
    def __init__(self,  lambda_L_genes):
        super(LaplacianLoss_Genes, self).__init__()
        self.lambda_L_genes = lambda_L_genes
        
    
    def forward(self, dhat, zhat,L_genes, percent_genes=1):
        if percent_genes < 1:
          num_used_genes = int(L_genes.size(0) * percent_genes)
          gene_indices = torch.randperm(L_genes.size(0))[:num_used_genes]
          # create a meshgrid
          grid_x_L, grid_y_L = torch.meshgrid(gene_indices, gene_indices)
          L_genes_subgenes = L_genes[grid_x_L, grid_y_L]
          D_subgenes = dhat[gene_indices,:]

        else:
          L_genes_subgenes = L_genes
          D_subgenes = dhat
          

        L_genes_subgenes = L_genes_subgenes.to(dhat.device)
        genes_prior = self.lambda_L_genes * torch.trace(torch.matmul(torch.matmul(torch.t(D_subgenes), L_genes_subgenes), D_subgenes) )
        return genes_prior
    

class LaplacianLoss(torch.nn.Module):
    def __init__(self, lambda_L_cells=None, lambda_L_genes=None, percent_genes=1):
        super(LaplacianLoss, self).__init__()
        self.lambda_L_cells = lambda_L_cells
        self.lambda_L_genes = lambda_L_genes
    
    def forward(self, dhat, zhat, L_cells=None, L_genes=None, percent_genes=1): # dhat corresponds to dictonary, zhat corresponds to code
        if L_genes != None:
            if percent_genes < 1:
              num_used_genes = int(L_genes.size(0) * percent_genes)
              gene_indices = torch.randperm(L_genes.size(0))[:num_used_genes]
              # create a meshgrid
              grid_x_L, grid_y_L = torch.meshgrid(gene_indices, gene_indices)
              L_genes_subgenes = L_genes[grid_x_L, grid_y_L]
              D_subgenes = dhat[gene_indices,:]
        
            else:
              L_genes_subgenes = L_genes
              D_subgenes = dhat
    
            genes_prior = self.lambda_L_genes * torch.trace(torch.matmul(torch.matmul(torch.t(D_subgenes), L_genes_subgenes.to(dhat.device)), D_subgenes) )
        else:
            genes_prior = torch.tensor(0)
        
        if L_cells != None:
            cell_prior = self.lambda_L_cells * torch.trace(torch.matmul(torch.matmul(torch.t(torch.squeeze(zhat, dim=-1)), L_cells), torch.squeeze(zhat, dim=-1) ) )
        else:
            cells_prior = torch.tensor(0)
        

    #   if torch.is_tensor(L_cells):
    #     L_cells = L_cells.to(dhat.device)
    #     cell_prior = self.lambda_L_cells * torch.trace(torch.matmul(torch.matmul(torch.t(torch.squeeze(zhat, dim=-1)), L_cells), torch.squeeze(zhat, dim=-1) ) )
      
        #print("Hey Shaye, Bahareh here! Be careful that this loss only output genes_prior and cell_perior is zero!")
        #prior_penalty = genes_prior + cells_prior
        prior_penalty = genes_prior
        return prior_penalty
        
class LaplacianLoss_original(torch.nn.Module):
    def __init__(self, lambda_L_cells, lambda_L_genes):
        super(LaplacianLoss_original, self).__init__()
        self.lambda_L_cells = lambda_L_cells
        self.lambda_L_genes = lambda_L_genes
    
    def forward(self, dhat, zhat, L_cells, L_genes, percent_genes): # dhat corresponds to dictonary, zhat corresponds to code
      if percent_genes < 1:
        num_used_genes = int(L_genes.size(0) * percent_genes)
        #print("num_used genes",num_used_genes )
        gene_indices = torch.randperm(L_genes.size(0))[:num_used_genes]
        #print("gene_indices",gene_indices.shape )
        # create a meshgrid
        grid_x_L, grid_y_L = torch.meshgrid(gene_indices, gene_indices)
        L_genes_subgenes = L_genes[grid_x_L, grid_y_L]
        #print('L genes sub',L_genes_subgenes.shape)
        D_subgenes = dhat[gene_indices,:]
        #print('D_subgenes',D_subgenes.shape, D_subgenes[:3,:3])

      else:
        L_genes_subgenes = L_genes
        D_subgenes = dhat

      L_genes_subgenes = L_genes_subgenes.to(dhat.device)
      L_cells = L_cells.to(dhat.device)
      genes_prior = self.lambda_L_genes * torch.trace(torch.matmul(torch.matmul(torch.t(D_subgenes), L_genes_subgenes), D_subgenes) )
      if self.lambda_L_cells != 0:
        cell_prior = self.lambda_L_cells * torch.trace(torch.matmul(torch.matmul(torch.t(torch.squeeze(zhat, dim=-1)), L_cells), torch.squeeze(zhat, dim=-1) ) )
      else:
        cell_prior = 0
      #print('genes prior', genes_prior)
      prior_penalty = genes_prior + cell_prior
      #print('cell_prior',cell_prior)
      return prior_penalty
      
class AE(torch.nn.Module):
    def __init__(self, params, W=None):
        super(AE, self).__init__()

        self.num_genes = params["num_genes"]
        self.num_modules = params["num_modules"]
        self.device = params["device"]
        self.code_l1_reg_net = params["code_l1_reg_net"] #lambda1
        self.enc_layers = params["enc_layers"] # unrolled iterations
        self.non_neg_code = params["non_neg_code"] # if true --> ReLU, if false --> shrinkige

        self.relu = torch.nn.ReLU()

        if W is None:
            W = torch.randn((self.num_genes, self.num_modules), device=self.device) # initialize dictionary
        W = F.normalize(W, p=2, dim=0) # each column has l2 norm = 1, axis=0 (column wise)

        self.register_parameter("W", torch.nn.Parameter(W)) # registering dictionary as trainable parameter
        self.register_buffer("alpha", torch.tensor(params["alpha"])) # registering step size as buffer -- callable as self.alpha

    def get_param(self, name):
        if name == 'all':
          return self.state_dict(keep_vars=True)
        else:
          return self.state_dict(keep_vars=True)[name] # way to return values from parameter dictionary

    def normalize(self):
        self.W.data = F.normalize(self.W.data, p=2, dim=0) # may want to normalize columns of of W 
    def normalize_less_one(self):
        m_norm = torch.norm(self.W.data, keepdim=True, dim=0) 
        m_norm[m_norm <= 1] = 1
        norm_mat = matrix / m_norm
        self.W.data = norm_mat

norm_mat = matrix / m_norm

    def nonlin(self, z):
        if self.non_neg_code:
            z = self.relu(z - self.code_l1_reg_net * self.alpha)
        else:
            z = self.relu(torch.abs(z) - self.code_l1_reg_net * self.alpha) * torch.sign(z)
        return z

    def encode(self, x):
        batch_size, device = x.shape[0], x.device
        zhat = torch.zeros(batch_size, self.num_modules, 1, device=device) # initializing z_0
        IplusWTW = torch.eye(self.num_modules, device=device) - self.alpha * torch.matmul(
            torch.t(self.W), self.W
        )
        WTx = self.alpha * torch.matmul(torch.t(self.W), x)
        for k in range(self.enc_layers):
            zhat = self.nonlin(torch.matmul(IplusWTW, zhat) + WTx)
            if self.enc_layers - k >= 10:
              zhat = zhat.clone().detach().requires_grad_(False)
        return zhat

    def decode(self, z_final):
        return torch.matmul(self.W, z_final)

    def forward(self, x):
        z = self.encode(x)
        xhat = self.decode(z)
        return xhat, z

class MyDataset(Dataset):
    def __init__(self, x_data):
        self.x_data = x_data
        #print(self.x_data.shape)
        # self.Laplacian_cells = Laplacian_cells.copy()
        #print(self.Laplacian_cells.shape)
        
    def __len__(self):
        return len(self.x_data)
    
    def __getitem__(self, index):
        x = torch.tensor(self.x_data[index], dtype=torch.float32)
        #print(x.shape)
        # Laplacian_cells_batch = torch.tensor(self.Laplacian_cells[np.ix_(index,index)], dtype=torch.float32)
        #print(Laplacian_cells_.shape)
        #Laplacian_genes = torch.tensor(self.Laplacian_genes, dtype=torch.float32)

        return x, index

#@jit(target='cuda')
def main():
  print(params['device'])
  # read in Laplacians
#   L_cells_guide_pooled = scipy.io.loadmat(processing_params['Laplacian_cells'][0])['L_manifold'].todense()
#   L_genes_guide_pooled = torch.tensor(scipy.io.loadmat(processing_params['Laplacian_genes'][0])['L_full_data'].todense(), dtype=torch.float32 )
  #L_cells_conventional = scipy.io.loadmat(processing_params['Laplacian_cells'][1])['L_manifold'].todense()
  #L_genes_conventional = torch.tensor(scipy.io.loadmat(processing_params['Laplacian_genes'][1])['L_full_data'].todense(), dtype=torch.float32 )
  
  print(f"processing {params['name_input_data']}...")



  overload_type = processing_params['overload_type'] # use this version by default

  X = np.load(processing_params['inputs_processed_X'][params['dataset_index']])
  logmeanexp = np.load(processing_params['logmeanexp'][params['dataset_index']])

  print('Loading expression matrix and indicator matrix... ')
  tens_X = torch.tensor(X, dtype=torch.float32)
  index_dict = torch.randint(low=0, high=X.shape[0], size =(params['num_modules'], ))
  W_init = torch.t(tens_X[index_dict,])
  print('Initial Dictionary Dimensions',W_init.shape)
  
  
  genes = pd.read_csv(processing_params['genes'][params['dataset_index']], sep = '\t')
  genes = genes[['features']]

  p_mat_pd = pd.read_csv(processing_params['input_perturbation_matrix'][params['dataset_index']], index_col = 0, delim_whitespace=True)

  # center rows of p_mat
  if overload_type == 'droplet':
      guides_per_cell = np.array(p_mat_pd.sum(axis = 0))
      p_mat_pd = p_mat_pd.divide(guides_per_cell[np.newaxis, :])

  # removing cells with no guides
  keep_cells = p_mat_pd.sum(axis = 0) > 0

  p_mat = np.asfortranarray(p_mat_pd.loc[:, keep_cells].T).astype(float)
  print('End processing ready for DL... ')

  params["num_genes"] = X.shape[1] # number of genes to use as dictionary
  params["num_data"] = X.shape[0] # number of cells to use in training, we're using all cells so that nothing goes into the validation set

  num_train = int(params["num_data"]  * params["train_val_split"])


  train_dataset = MyDataset(np.expand_dims(X, axis=2))

  # make dataloader
  train_loader = torch.utils.data.DataLoader(
  train_dataset,
  shuffle=params["shuffle"],
  batch_size=params["batch_size"],
  num_workers=params["num_workers"],
  )
  
  print('Initializing dictionary to random columns of data...')
  

  print('Initializing pytorch model...')
  net = AE(params, W=W_init.to(params['device']))

  print('Initializing optimizer...')
  optimizer_lasso = torch.optim.Adam(net.parameters(), lr=params["lr_lasso"], eps=params['eps_Adam'])
  optimizer_laplacian = torch.optim.Adam(net.parameters(), lr=params["lr_laplacian"], eps=params['eps_Adam'] )

  print('Initializing loss function in model...')
  lasso_loss_fcn = LassoLoss(params['code_l1_reg_loss'])
  laplacian_loss_fcn = LaplacianLoss(lambda_L_cells=params['lambda_L_cells_reg'], lambda_L_genes=params['lambda_L_genes_reg'])
  #laplacian_loss_fcn = LaplacianLoss_original(lambda_L_cells=params['lambda_L_cells_reg'], lambda_L_genes=params['lambda_L_genes_reg'])
  #laplacian_loss_fcn_genes = LaplacianLoss_Genes(lambda_L_genes=params['lambda_L_genes_reg']) #dhat, zhat,L_genes=False, percent_genes=1
  print('Setting device...')
  device = torch.device(params['device'])

  print('Setting Scheduler...')
  #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)
  scheduler_lasso = torch.optim.lr_scheduler.MultiStepLR(optimizer_lasso, milestones=params['milestones'], gamma=0.1)
  scheduler_laplacian = torch.optim.lr_scheduler.MultiStepLR(optimizer_laplacian, milestones=params['milestones'], gamma=0.1)

  with open(f"{tensorboardx_path}/dictionary_{params['name_input_data']}_params_dict.pickle", 'wb') as handle:
    pickle.dump(params, handle, protocol=pickle.HIGHEST_PROTOCOL)
    
##   initial_params = net.get_param('all').copy()
  print('Getting Initial loss...')
  net.eval()
  lasso_loss_init = []
  laplacian_loss_init = []
  for idx, (x, index) in tqdm(enumerate(train_loader), disable=False):
      x = x.to(device) # x should contain 5 tensors or tensors the size of your batch
      xhat, z = net(x) # automatically runs forward
      index_L = index.tolist()
      lasso_loss = lasso_loss_fcn(x, xhat, z) # calls forward
      if args.laplacian_genes and not args.laplacian_cells:
          laplacian_loss = laplacian_loss_fcn(dhat= net.get_param('W'), zhat=z, L_genes=L_genes, percent_genes = params['percent_genes_train'])
      if not args.laplacian_genes and args.laplacian_cells:
          Laplacian_cells_batch = torch.tensor(L_cells[np.ix_(index_L,index_L)], dtype=torch.float32)
          laplacian_loss = laplacian_loss_fcn(dhat= net.get_param('W'), zhat=z, L_cells=Laplacian_cells_batch, percent_genes = params['percent_genes_train'])

      if args.laplacian_genes and args.laplacian_cells:
          Laplacian_cells_batch = torch.tensor(L_cells[np.ix_(index_L,index_L)], dtype=torch.float32)
          laplacian_loss = laplacian_loss_fcn(dhat= net.get_param('W'), zhat=z, L_cells=Laplacian_cells_batch, L_genes=L_genes, percent_genes = params['percent_genes_train'])

        # loss = lasso_loss + laplacian_loss
        # Hey Shaye, Bahareh here, in this line you won't append lasso_loss if laplacian is turned off. be careful and fix.
      if args.laplacian_genes or args.laplacian_cells:
          laplacian_loss_init.append(laplacian_loss.item())
      
      lasso_loss_init.append(lasso_loss.item())
      
  params[f"lasso_{params['name_input_data']}_initial_loss"] = np.mean(lasso_loss_init)
  print('initial_loss', np.mean(lasso_loss_init))

  if args.laplacian_genes or args.laplacian_cells:
    params[f"laplacian_{params['name_input_data']}_initial_loss"] = np.mean(laplacian_loss_init)
    params[f"all_{params['name_input_data']}_initial_loss"] = np.mean(lasso_loss_init) + np.mean(laplacian_loss_init)

  print("Check if GPU is being used")
  if torch.cuda.is_available():
    print("PASS! GPU is used for training!")
  else:
    print("WARNING: CPU!!!!")

  print('Training Network')
  lasso_mean_loss_per_epoch = []
  laplacian_mean_loss_per_epoch = []
  net.train() # allowing gradient to be trainable
  counter = 0
  for epoch in tqdm(range(params["num_epochs"]), disable=False):
    lasso_loss_per_epoch = []
    laplacian_loss_per_epoch = []
    
    # step scheduler at each epoch
    if epoch > 0:
      scheduler_lasso.step()
      scheduler_laplacian.step()

    for idx, (x, index) in tqdm(enumerate(train_loader), disable=True):


      x = x.to(device) # x should contain 5 tensors or tensors the size of your batch
      xhat, z = net(x) # automatically runs forward
      index_L = index.tolist()
      if args.laplacian_genes and not args.laplacian_cells:
        laplacian_loss = laplacian_loss_fcn(dhat= net.get_param('W'), zhat=z, L_genes=L_genes, percent_genes = params['percent_genes_train'])

      if not args.laplacian_genes and args.laplacian_cells:
        print('Warning: does not contain laplacian genes but contains laplacian cells')
        Laplacian_cells_batch = torch.tensor(L_cells[np.ix_(index_L,index_L)], dtype=torch.float32)
        laplacian_loss = laplacian_loss_fcn(dhat= net.get_param('W'), zhat=z, L_cells=Laplacian_cells_batch, percent_genes = params['percent_genes_train'])

      if args.laplacian_genes and args.laplacian_cells:
        print('Warning: contains laplacian cells')
        Laplacian_cells_batch = torch.tensor(L_cells[np.ix_(index_L,index_L)], dtype=torch.float32)
        laplacian_loss = laplacian_loss_fcn(dhat= net.get_param('W'), zhat=z, L_cells=Laplacian_cells_batch, L_genes=L_genes, percent_genes = params['percent_genes_train'])
      
      if not args.laplacian_genes and not args.laplacian_cells:
        #print('Warning, no laplacians')
        laplacian_loss = torch.tensor(0)
        #print('warning')
      
      #Laplacian_cells_batch = torch.tensor(L_cells_guide_pooled[np.ix_(index_L,index_L)], dtype=torch.float32)
      #laplacian_loss = laplacian_loss_fcn(dhat= net.get_param('W'), zhat=z, L_cells=Laplacian_cells_batch, L_genes=L_genes_guide_pooled, percent_genes = params['percent_genes_train'])
      
      lasso_loss = lasso_loss_fcn(x, xhat, z) # calls forward
      
      
      #loss = lasso_loss + laplacian_loss

      lasso_loss_per_epoch.append(lasso_loss.item())
      laplacian_loss_per_epoch.append(laplacian_loss.item())

      counter += 1
      writer.add_scalar('lasso_loss', lasso_loss.item(), counter)
      writer.add_scalar('laplacian_loss', laplacian_loss.item(), counter)
      writer.add_scalar('total_loss', (laplacian_loss.item()+lasso_loss.item()), counter)
      
      if args.delay_Adam_opt and epoch == 50:
          optimizer_lasso = torch.optim.Adam(net.parameters(), lr=params["lr_lasso"], eps=params["delay_eps_Adam"])
          optimizer_laplacian = torch.optim.Adam(net.parameters(), lr=params["lr_laplacian"], eps=params["delay_eps_Adam"] )
          scheduler_lasso = torch.optim.lr_scheduler.MultiStepLR(optimizer_lasso, milestones=[10,30], gamma=0.1)
          scheduler_laplacian = torch.optim.lr_scheduler.MultiStepLR(optimizer_laplacian, milestones=[10,30], gamma=0.1)
          
      
      
      if args.laplacian_genes or args.laplacian_cells:
          if idx % params['laplacian_update_period'] == 0:
            optimizer_laplacian.zero_grad() # set gradients equal to -
            laplacian_loss.backward(retain_graph=True)
            optimizer_laplacian.step() # gradient steps to update W 
          
          
      if idx % params['lasso_update_period'] == 0:
        optimizer_lasso.zero_grad()
        lasso_loss.backward()
        optimizer_lasso.step()  
      if params["norm_below_one"]:
        
      if params["normalize"]:
        net.normalize() # normalize dictionary after every update (post backprop -- where the dictionary gets updated)
      if params["normalize_greater_one"]:
        net.normalize_less_one()
    lasso_mean_loss_per_epoch.append(np.mean(lasso_loss_per_epoch))
    laplacian_mean_loss_per_epoch.append(np.mean(laplacian_loss_per_epoch))


  params[f"lasso_{params['name_input_data']}_mean_loss_per_epoch_train"] = np.mean(lasso_mean_loss_per_epoch)
  params[f"laplacian_{params['name_input_data']}_mean_loss_per_epoch_train"] = np.mean(laplacian_loss_per_epoch)
  params[f"all_{params['name_input_data']}_mean_loss_per_epoch_train"] = np.mean(lasso_mean_loss_per_epoch) + np.mean(laplacian_loss_per_epoch)

  # extract code after training -- this code is properly ordered 
  test_loader = torch.utils.data.DataLoader(
  train_dataset,
  shuffle=False,
  #shuffle=params["shuffle"],
  batch_size=params["batch_size"],
  num_workers=params["num_workers"],
  )

  print('Recovering trained dictionary and code, test model')
  net.eval()
  lasso_loss_init = []
  laplacian_loss_init = []
  for idx, (x, index) in tqdm(enumerate(train_loader), disable=False):
    x = x.to(device) # x should contain 5 tensors or tensors the size of your batch
    xhat, z = net(x) # automatically runs forward
    index_L = index.tolist()
    
    # extract code
    if idx == 0:
      code = z
    else: 
      code = torch.vstack((code,z))

    lasso_loss = lasso_loss_fcn(x, xhat, z) # calls forward
    if args.laplacian_genes and not args.laplacian_cells:
      laplacian_loss = laplacian_loss_fcn(dhat= net.get_param('W'), zhat=z, L_genes=L_genes, percent_genes = params['percent_genes_train'])

    if not args.laplacian_genes and args.laplacian_cells:
      Laplacian_cells_batch = torch.tensor(L_cells[np.ix_(index_L,index_L)], dtype=torch.float32)
      laplacian_loss = laplacian_loss_fcn(dhat= net.get_param('W'), zhat=z, L_cells=Laplacian_cells_batch, percent_genes = params['percent_genes_train'])

    if args.laplacian_genes and args.laplacian_cells:
      Laplacian_cells_batch = torch.tensor(L_cells[np.ix_(index_L,index_L)], dtype=torch.float32)
      laplacian_loss = laplacian_loss_fcn(dhat= net.get_param('W'), zhat=z, L_cells=Laplacian_cells_batch, L_genes=L_genes, percent_genes = params['percent_genes_train'])

    # loss = lasso_loss + laplacian_loss
    if args.laplacian_genes or args.laplacian_cells:
      laplacian_loss_init.append(laplacian_loss.item())
    
    lasso_loss_init.append(lasso_loss.item())
      
  params[f"lasso_{params['name_input_data']}_test_loss"] = np.mean(lasso_loss_init)

  if args.laplacian_genes or args.laplacian_cells:
    params[f"laplacian_{params['name_input_data']}_test_loss"] = np.mean(laplacian_loss_init)
    params[f"all_{params['name_input_data']}_test_loss"] = np.mean(lasso_loss_init) + np.mean(laplacian_loss_init)

  print('Obtain final dictionary and code and convert to numpy')
  # Get final dictionary and code and detach, return from gpu to cpu and then convert to numpy
  W = net.get_param('W')
  Z = code

  W_cpu_numpy = W.detach().cpu().numpy()
  print("w", W_cpu_numpy.shape)
  Z_cpu_numpy = np.squeeze(Z.detach().cpu().numpy(),axis=2)
  print('Z',Z_cpu_numpy.shape )
  np.save(f"{tensorboardx_path}/dictionary_{params['name_input_data']}_final_unrolled.npy", W_cpu_numpy)
  np.save(f"{tensorboardx_path}/code_{params['name_input_data']}_final_unrolled.npy", Z_cpu_numpy)

if __name__ == "__main__":
    
  print(params["device"])
  print(params["enc_layers"])
  
  #params["device"] = "cuda:0" if torch.cuda.is_available() else "cpu"
  
  #print(params["device"], "haha")
  print("",params["name_input_data"])
  print(processing_params['inputs_processed_X'][params['dataset_index']])
  print(processing_params['logmeanexp'][params['dataset_index']])
  print(processing_params['input_perturbation_matrix'][params['dataset_index']])
  print("number of epochs", params['num_epochs'])
  print("milestones", params['milestones'])
  
  print("lasso update period", params["lasso_update_period"])
    
  main()

# %reload_ext tensorboard
# %tensorboard --logdir /content/drive/MyDrive/AM231_project/data/logs/tensorboard/train/ --port=6007